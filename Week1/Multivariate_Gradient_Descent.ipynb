{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n_ZwYtYkkhMw"
   },
   "source": [
    "In this part of the assignment you have to implement multivariate gradient descent to find the minimas (local and global) of the given function:\n",
    "Note : you can find different minimas by changing your initialisation.  \n",
    "$f(x) = x^4 + x^2y^2 - y^2 + y^4 + 6$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "AGC301Ynkcth"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged after 0 iterations.\n",
      "Starting point: (0, 0) -> Minima: [0. 0.], Function value: 6.0\n",
      "Converged after 845 iterations.\n",
      "Starting point: (5, 5) -> Minima: [-9.81578278e-05 -7.07106774e-01], Function value: 5.75000000481748\n",
      "Converged after 845 iterations.\n",
      "Starting point: (-5, -5) -> Minima: [9.81578278e-05 7.07106774e-01], Function value: 5.75000000481748\n",
      "Converged after 845 iterations.\n",
      "Starting point: (5, -5) -> Minima: [-9.81578278e-05  7.07106774e-01], Function value: 5.75000000481748\n",
      "Converged after 845 iterations.\n",
      "Starting point: (-5, 5) -> Minima: [ 9.81578278e-05 -7.07106774e-01], Function value: 5.75000000481748\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the function\n",
    "def f(x, y):\n",
    "    return x**4 + x**2 * y**2 - y**2 + y**4 + 6\n",
    "\n",
    "# Define the partial derivatives\n",
    "def grad_f(x, y):\n",
    "    df_dx = 4*x**3 + 2*x*y**2\n",
    "    df_dy = 2*x**2*y - 2*y + 4*y**3\n",
    "    return np.array([df_dx, df_dy])\n",
    "\n",
    "# Gradient Descent implementation\n",
    "def gradient_descent(initial_point, learning_rate=0.01, tolerance=1e-6, max_iterations=10000):\n",
    "    point = np.array(initial_point, dtype=float)\n",
    "    for i in range(max_iterations):\n",
    "        grad = grad_f(point[0], point[1])\n",
    "        new_point = point - learning_rate * grad\n",
    "        if np.linalg.norm(new_point - point) < tolerance:\n",
    "            print(f\"Converged after {i} iterations.\")\n",
    "            return new_point, f(new_point[0], new_point[1])\n",
    "        point = new_point\n",
    "    print(\"Reached maximum iterations.\")\n",
    "    return point, f(point[0], point[1])\n",
    "\n",
    "# Find minima by initializing at different points\n",
    "initial_points = [(0, 0), (5, 5), (-5, -5), (5, -5), (-5, 5)]\n",
    "for initial in initial_points:\n",
    "    minima, min_value = gradient_descent(initial)\n",
    "    print(f\"Starting point: {initial} -> Minima: {minima}, Function value: {min_value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
